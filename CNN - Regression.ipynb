{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train_path = \"img_dataset/\"\n",
    "img_test_path = \"img_dataset_teste/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagens treino: 1495\n",
      "Imagens teste: 16\n"
     ]
    }
   ],
   "source": [
    "imgs_train = []\n",
    "imgs_test = []\n",
    "\n",
    "for i in range(1495):\n",
    "    img = cv2.imread(img_train_path + \"raiz_\" + str(i) + \".bmp\", 0)\n",
    "    imgs_train.append(np.reshape(np.array(img), (128, 128, 1)))\n",
    "\n",
    "images_train = np.asarray(imgs_train) \n",
    "\n",
    "for i in range(16):\n",
    "    img = cv2.imread(img_test_path + \"raiz_\" + str(i) + \".bmp\", 0)\n",
    "    imgs_test.append(np.reshape(np.array(img), (128, 128, 1)))\n",
    "\n",
    "images_test = np.asarray(imgs_test)\n",
    "\n",
    "print(\"Imagens treino:\", len(imgs_train))\n",
    "print(\"Imagens teste:\", len(imgs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rótulos: 1495 [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "labels = np.genfromtxt('percentuais.csv', delimiter=',')\n",
    "print(\"Rótulos:\", len(labels), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1 - LR = 0.0001 e EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 126, 126, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 126, 126, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 59, 59, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 59, 59, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 64)        165952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 21, 21, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               3200500   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,445,057\n",
      "Trainable params: 3,444,801\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Train on 1345 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      " - 37s - loss: 2.8231 - acc: 0.1182 - mean_squared_error: 2.8231 - val_loss: 0.2350 - val_acc: 0.1400 - val_mean_squared_error: 0.2350\n",
      "Epoch 2/20\n",
      " - 33s - loss: 1.0783 - acc: 0.1517 - mean_squared_error: 1.0783 - val_loss: 0.1056 - val_acc: 0.1867 - val_mean_squared_error: 0.1056\n",
      "Epoch 3/20\n",
      " - 33s - loss: 0.7134 - acc: 0.1695 - mean_squared_error: 0.7134 - val_loss: 0.0500 - val_acc: 0.2200 - val_mean_squared_error: 0.0500\n",
      "Epoch 4/20\n",
      " - 32s - loss: 0.4536 - acc: 0.2052 - mean_squared_error: 0.4536 - val_loss: 0.0288 - val_acc: 0.2333 - val_mean_squared_error: 0.0288\n",
      "Epoch 5/20\n",
      " - 32s - loss: 0.3445 - acc: 0.2268 - mean_squared_error: 0.3445 - val_loss: 0.0223 - val_acc: 0.2333 - val_mean_squared_error: 0.0223\n",
      "Epoch 6/20\n",
      " - 32s - loss: 0.2659 - acc: 0.2498 - mean_squared_error: 0.2659 - val_loss: 0.0264 - val_acc: 0.2333 - val_mean_squared_error: 0.0264\n",
      "Epoch 7/20\n",
      " - 32s - loss: 0.2295 - acc: 0.2602 - mean_squared_error: 0.2295 - val_loss: 0.0295 - val_acc: 0.2333 - val_mean_squared_error: 0.0295\n",
      "Epoch 8/20\n",
      " - 32s - loss: 0.1919 - acc: 0.2721 - mean_squared_error: 0.1919 - val_loss: 0.0249 - val_acc: 0.2333 - val_mean_squared_error: 0.0249\n",
      "Epoch 9/20\n",
      " - 32s - loss: 0.1534 - acc: 0.2818 - mean_squared_error: 0.1534 - val_loss: 0.0274 - val_acc: 0.2333 - val_mean_squared_error: 0.0274\n",
      "Epoch 10/20\n",
      " - 32s - loss: 0.1271 - acc: 0.2855 - mean_squared_error: 0.1271 - val_loss: 0.0272 - val_acc: 0.2333 - val_mean_squared_error: 0.0272\n",
      "Epoch 11/20\n",
      " - 32s - loss: 0.1169 - acc: 0.2944 - mean_squared_error: 0.1169 - val_loss: 0.0315 - val_acc: 0.2333 - val_mean_squared_error: 0.0315\n",
      "Epoch 12/20\n",
      " - 32s - loss: 0.0936 - acc: 0.2981 - mean_squared_error: 0.0936 - val_loss: 0.0331 - val_acc: 0.2333 - val_mean_squared_error: 0.0331\n",
      "Epoch 13/20\n",
      " - 32s - loss: 0.0929 - acc: 0.3056 - mean_squared_error: 0.0929 - val_loss: 0.0291 - val_acc: 0.2333 - val_mean_squared_error: 0.0291\n",
      "Epoch 14/20\n",
      " - 32s - loss: 0.0641 - acc: 0.3026 - mean_squared_error: 0.0641 - val_loss: 0.0248 - val_acc: 0.2333 - val_mean_squared_error: 0.0248\n",
      "Epoch 15/20\n",
      " - 32s - loss: 0.0750 - acc: 0.3078 - mean_squared_error: 0.0750 - val_loss: 0.0269 - val_acc: 0.2333 - val_mean_squared_error: 0.0269\n",
      "Epoch 16/20\n",
      " - 33s - loss: 0.0667 - acc: 0.3138 - mean_squared_error: 0.0667 - val_loss: 0.0269 - val_acc: 0.2333 - val_mean_squared_error: 0.0269\n",
      "Epoch 17/20\n",
      " - 32s - loss: 0.0538 - acc: 0.3130 - mean_squared_error: 0.0538 - val_loss: 0.0261 - val_acc: 0.2333 - val_mean_squared_error: 0.0261\n",
      "Epoch 18/20\n",
      " - 32s - loss: 0.0519 - acc: 0.3138 - mean_squared_error: 0.0519 - val_loss: 0.0217 - val_acc: 0.2333 - val_mean_squared_error: 0.0217\n",
      "Epoch 19/20\n",
      " - 33s - loss: 0.0555 - acc: 0.3160 - mean_squared_error: 0.0555 - val_loss: 0.0189 - val_acc: 0.2333 - val_mean_squared_error: 0.0189\n",
      "Epoch 20/20\n",
      " - 32s - loss: 0.0456 - acc: 0.3152 - mean_squared_error: 0.0456 - val_loss: 0.0198 - val_acc: 0.2333 - val_mean_squared_error: 0.0198\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(32, (5, 5), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (9, 9), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(500, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(.25),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(Adam(lr=.00001), loss='mean_squared_error', metrics=['accuracy', 'mean_squared_error'])\n",
    "\n",
    "model1.fit(images_train, labels, validation_split=0.1, epochs=20, batch_size=5, verbose=2)\n",
    "\n",
    "model1.save(\"2model1_lr-0001_ep-10.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 2 - LR = 0.0001 e EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(32, (5, 5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (9, 9), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(500, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(Adam(lr=.0001), loss='mean_squared_error', metrics=['accuracy', 'mean_squared_error'])\n",
    "\n",
    "model2.fit(images_train, labels, validation_split=0.1, epochs=20, batch_size=5, verbose=2)\n",
    "\n",
    "model2.save(\"2model2_lr-0001_ep-20.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 3 - LR=0.00001 e EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(32, (5, 5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (9, 9), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(500, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model3.summary()\n",
    "\n",
    "model3.compile(Adam(lr=.00001), loss='mean_squared_error', metrics=['accuracy', 'mean_squared_error'])\n",
    "\n",
    "model3.fit(images_train, labels, validation_split=0.1, epochs=10, batch_size=5, verbose=2)\n",
    "\n",
    "model3.save(\"2model3_lr-00001_ep-10.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MODEL 4 - LR=0.00001 e EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(32, (5, 5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (9, 9), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(500, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model4.summary()\n",
    "\n",
    "model4.compile(Adam(lr=.00001), loss='mean_squared_error', metrics=['accuracy', 'mean_squared_error'])\n",
    "\n",
    "model4.fit(images_train, labels, validation_split=0.1, epochs=20, batch_size=5, verbose=2)\n",
    "\n",
    "model4.save(\"2model4_lr-00001_ep-20.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model1.predict(images_test, batch_size=5)\n",
    "predictions2 = model2.predict(images_test, batch_size=5)\n",
    "predictions3 = model3.predict(images_test, batch_size=5)\n",
    "predictions4 = model4.predict(images_test, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = []\n",
    "\n",
    "for i in range(16):\n",
    "    row = []\n",
    "    row.append(i)\n",
    "    row.append(predictions1[i, 0])\n",
    "    row.append(predictions2[i, 0])\n",
    "    row.append(predictions3[i, 0])\n",
    "    row.append(predictions4[i, 0])\n",
    "    percents.append(row)\n",
    "    print(row)\n",
    "\n",
    "#print(percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"predictions.csv\", percents, delimiter=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
